name: AI Safety Monitor

on:
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  workflow_dispatch:
    inputs:
      run_type:
        description: 'Type of run'
        required: true
        default: 'standard'
        type: choice
        options:
          - standard
          - verbose
          - debug

env:
  DOCKER_COMPOSE_PROJECT: ai-safety-monitor-${{ github.run_number }}

jobs:
  monitor:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Debug workspace structure
        run: |
          echo "üìÅ Current directory structure:"
          ls -la
          echo ""
          echo "üìÅ .github directory structure:"
          ls -la .github/ || echo "No .github directory"
          echo ""
          echo "üìÅ Scripts directory structure:"
          ls -la .github/scripts/ || echo "No scripts directory"

      - name: Download previous monitoring data
        uses: actions/download-artifact@v4
        with:
          name: monitoring-data
          path: data/
        continue-on-error: true

      - name: Verify downloaded artifacts
        run: |
          echo "üì¶ Verifying downloaded artifacts..."
          echo "Artifacts downloaded to data/:"
          find data/ -type f -name "*" 2>/dev/null | while read file; do
            size=$(wc -c < "$file" 2>/dev/null || echo "0")
            echo "  - $file ($size bytes)"
          done || echo "No artifacts found"
          
          echo "Checking for changedetection.db:"
          if [ -f "data/datastore/changedetection.db" ]; then
            db_size=$(wc -c < "data/datastore/changedetection.db")
            echo "‚úÖ changedetection.db found: $db_size bytes"
          else
            echo "‚ùå changedetection.db not found"
          fi

      - name: Restore data permissions and structure
        run: |
          echo "üîß Setting up data directories..."
          mkdir -p data/datastore data/reports data/logs
          chmod -R 777 data/ || true
          
          echo "üìä Existing data content:"
          find data/ -type f -name "*.json" | head -10 || echo "No existing data files"
          echo "Datastore files:"
          ls -la data/datastore/ 2>/dev/null || echo "No datastore files yet"

      - name: Verify prerequisites and scripts
        run: |
          echo "üîß Checking prerequisites..."
          command -v docker && echo "‚úÖ docker found: $(docker --version)" || (echo "‚ùå docker not found"; exit 1)
          
          # Check for docker compose (plugin) or docker-compose (standalone)
          if docker compose version >/dev/null 2>&1; then
            echo "‚úÖ docker compose (plugin) found: $(docker compose version | head -1)"
            echo "DOCKER_COMPOSE_CMD=docker compose" >> $GITHUB_ENV
          elif command -v docker-compose >/dev/null 2>&1; then
            echo "‚úÖ docker-compose (standalone) found: $(docker-compose --version)"
            echo "DOCKER_COMPOSE_CMD=docker-compose" >> $GITHUB_ENV
          else
            echo "‚ùå Neither docker compose nor docker-compose found"
            exit 1
          fi
          
          command -v curl && echo "‚úÖ curl found: $(curl --version | head -1)" || (echo "‚ùå curl not found"; exit 1)
          command -v jq && echo "‚úÖ jq found: $(jq --version)" || (echo "‚ùå jq not found"; exit 1)
          
          echo ""
          echo "üìú Checking required scripts..."
          required_scripts=(
            ".github/scripts/config.sh"
            ".github/scripts/logger.sh"
            ".github/scripts/wait-for-service.sh"
            ".github/scripts/extract-api-key.sh"
          )
          
          for script in "${required_scripts[@]}"; do
            if [ ! -f "$script" ]; then
              echo "‚ùå Required script not found: $script"
              exit 1
            fi
            chmod +x "$script"
            echo "‚úÖ $script is executable"
          done
          
          echo "‚úÖ All prerequisites and scripts available"

      - name: Setup environment
        run: |
          # Source config and make available to subsequent steps
          set -a  # Automatically export all variables
          source .github/scripts/config.sh
          set +a
          
          # Export all config variables for subsequent steps
          {
            echo "CHANGEDETECTION_PORT=$CHANGEDETECTION_PORT"
            echo "CHANGEDETECTION_HOST=$CHANGEDETECTION_HOST"
            echo "CHANGEDETECTION_URL=$CHANGEDETECTION_URL"
            echo "CHANGEDETECTION_CONTAINER_URL=$CHANGEDETECTION_CONTAINER_URL"
            echo "MAX_WAIT_ATTEMPTS=$MAX_WAIT_ATTEMPTS"
            echo "WAIT_INTERVAL=$WAIT_INTERVAL"
            echo "MAX_MONITOR_ATTEMPTS=$MAX_MONITOR_ATTEMPTS"
            echo "RESTART_DELAY=$RESTART_DELAY"
            echo "DATASTORE_FILE=$DATASTORE_FILE"
            echo "LOCAL_DATASTORE=$LOCAL_DATASTORE"
            echo "API_KEY_FIELD=$API_KEY_FIELD"
            echo "LOG_DIR=$LOG_DIR"
            echo "DOCKER_LOG_DIR=$DOCKER_LOG_DIR"
            echo "REPORT_DIR=$REPORT_DIR"
          } >> $GITHUB_ENV
          
          echo "‚úÖ Environment configured"
          echo "Service URL: $CHANGEDETECTION_URL"
          echo "Docker Compose Command: $DOCKER_COMPOSE_CMD"

      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          driver: docker-container
          buildkitd-flags: --debug

      - name: Cache Python packages
        uses: actions/cache@v4
        id: pip-cache
        with:
          path: |
            ~/.cache/pip
            /tmp/pip-cache
            **/__pycache__
            **/.pytest_cache
          key: ${{ runner.os }}-python-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-python-

      - name: Cache Docker layers
        uses: actions/cache@v4
        id: docker-cache
        with:
          path: |
            ~/.cache/docker
            /tmp/.buildx-cache
            /var/lib/docker
          key: ${{ runner.os }}-docker-${{ hashFiles('docker-compose.yml', 'Dockerfile', 'requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-docker-

      - name: Verify cache status
        run: |
          echo "üîç Cache Status:"
          echo "Python cache hit: ${{ steps.pip-cache.outputs.cache-hit }}"
          echo "Docker cache hit: ${{ steps.docker-cache.outputs.cache-hit }}"
          
          if [[ "${{ steps.docker-cache.outputs.cache-hit }}" != 'true' ]]; then
            echo "Docker cache miss - this is normal for first run or when files change"
          else
            echo "‚úÖ Docker cache hit - layers will be reused"
          fi

      - name: Build and start services
        run: |
          source .github/scripts/logger.sh
          
          log_info "Building metadata-monitor image with cache..."
          $DOCKER_COMPOSE_CMD build --progress plain --build-arg BUILDKIT_INLINE_CACHE=1 metadata-monitor
          
          log_info "Starting changedetection and playwright services..."
          $DOCKER_COMPOSE_CMD -p $DOCKER_COMPOSE_PROJECT up -d changedetection playwright-chrome
          
          log_success "Services started successfully"

      - name: Wait for basic connectivity
        run: |
          .github/scripts/wait-for-service.sh "$CHANGEDETECTION_URL" "$MAX_WAIT_ATTEMPTS" "$WAIT_INTERVAL"

      - name: Extract API key from changedetection datastore
        id: extract-api-key
        run: |
          API_KEY=$(.github/scripts/extract-api-key.sh)
          echo "CHANGEDETECTION_API_KEY=$API_KEY" >> $GITHUB_ENV
          echo "api_key=$API_KEY" >> $GITHUB_OUTPUT
          echo "::add-mask::$API_KEY"

      - name: Test systeminfo endpoint
        env:
          API_KEY: ${{ steps.extract-api-key.outputs.api_key }}
        run: |
          source .github/scripts/logger.sh
          
          log_info "Testing systeminfo endpoint..."
          
          response=$(curl -s -X GET "$CHANGEDETECTION_URL/api/v1/systeminfo" \
            -H "x-api-key: $API_KEY" \
            -w "\n%{http_code}")
          
          http_code=$(echo "$response" | tail -1)
          
          if [ "$http_code" = "200" ]; then
            log_success "systeminfo endpoint returned HTTP 200"
          else
            log_failure "systeminfo endpoint returned HTTP $http_code"
            log_info "Response body:"
            echo "$response" | head -n -1
            exit 1
          fi

      - name: Prepare directories
        run: |
          source .github/scripts/logger.sh
          
          log_info "Creating required directories..."
          mkdir -p "$REPORT_DIR" "$LOG_DIR" "$DOCKER_LOG_DIR" "data/datastore" "data/reports"
          chmod -R 777 "$REPORT_DIR" "$LOG_DIR" "$DOCKER_LOG_DIR" "data"
          log_success "Directories prepared"

      - name: Check for existing data and set first run flag
        id: check-first-run
        run: |
          echo "üîç Checking for existing monitoring data..."
          
          # Check what artifacts were actually downloaded
          echo "üìÅ Downloaded artifacts in data/:"
          find data/ -type f -name "*" 2>/dev/null | head -20 || echo "No data files found"
          
          # Check for changedetection database (the actual datastore file)
          if [ -f "data/datastore/changedetection.db" ]; then
            echo "üìÅ Found changedetection.db datastore file"
            db_size=$(wc -c < "data/datastore/changedetection.db")
            echo "Datastore size: $db_size bytes"
            
            # Check if it's a valid SQLite database
            if command -v sqlite3 >/dev/null 2>&1; then
              if sqlite3 "data/datastore/changedetection.db" "SELECT name FROM sqlite_master WHERE type='table';" 2>/dev/null | grep -q watches; then
                echo "‚úÖ Valid changedetection database with watches table"
                echo "FIRST_RUN=false" >> $GITHUB_ENV
                echo "first_run=false" >> $GITHUB_OUTPUT
                echo "‚úÖ Continuing from existing datastore"
                exit 0
              else
                echo "‚ö†Ô∏è File exists but not a valid changedetection database"
              fi
            elif [ "$db_size" -gt 10000 ]; then  # More reasonable minimum size
              echo "‚úÖ Large datastore file ($db_size bytes), assuming valid"
              echo "FIRST_RUN=false" >> $GITHUB_ENV
              echo "first_run=false" >> $GITHUB_OUTPUT
              echo "‚úÖ Continuing from existing datastore"
              exit 0
            else
              echo "‚ö†Ô∏è Datastore file exists but is too small ($db_size bytes)"
            fi
          fi
          
          # Check for any previous monitoring reports
          if find data/reports/ -name "cycle_*.json" -type f 2>/dev/null | grep -q .; then
            report_count=$(find data/reports/ -name "cycle_*.json" -type f 2>/dev/null | wc -l)
            echo "Previous cycle reports found: $report_count"
            echo "FIRST_RUN=false" >> $GITHUB_ENV
            echo "first_run=false" >> $GITHUB_OUTPUT
            echo "‚úÖ Continuing from previous run with $report_count cycle reports"
          else
            echo "FIRST_RUN=true" >> $GITHUB_ENV
            echo "first_run=true" >> $GITHUB_OUTPUT
            echo "üÜï First run detected - no previous data found"
          fi

      - name: Verify first run flag
        run: |
          echo "First run flag from env: $FIRST_RUN"
          echo "First run flag from step output: ${{ steps.check-first-run.outputs.first_run }}"

      - name: Run monitoring cycle
        env:
          API_KEY: ${{ steps.extract-api-key.outputs.api_key }}
          VERBOSE: ${{ github.event.inputs.run_type == 'verbose' || github.event.inputs.run_type == 'debug' }}
          DEBUG: ${{ github.event.inputs.run_type == 'debug' }}
          GOOGLE_SHEETS_TYPE: ${{ secrets.GOOGLE_SHEETS_TYPE }}
          GOOGLE_SHEETS_PROJECT_ID: ${{ secrets.GOOGLE_SHEETS_PROJECT_ID }}
          GOOGLE_SHEETS_PRIVATE_KEY_ID: ${{ secrets.GOOGLE_SHEETS_PRIVATE_KEY_ID }}
          GOOGLE_SHEETS_PRIVATE_KEY: ${{ secrets.GOOGLE_SHEETS_PRIVATE_KEY }}
          GOOGLE_SHEETS_CLIENT_EMAIL: ${{ secrets.GOOGLE_SHEETS_CLIENT_EMAIL }}
          GOOGLE_SHEETS_CLIENT_ID: ${{ secrets.GOOGLE_SHEETS_CLIENT_ID }}
        run: |
          set -e
          source .github/scripts/logger.sh

          # Get the first_run value directly from the step output
          FIRST_RUN="${{ steps.check-first-run.outputs.first_run }}"
          
          log_info "First run status: $FIRST_RUN"
          log_info "Existing data check completed"
          
          # Enable debug mode if requested
          if [ "$DEBUG" = "true" ]; then
            set -x
            export PS4='+ $(date -u "+%H:%M:%S") '
            log_info "DEBUG mode enabled"
          fi
          
          log_info "Performing final health check..."
          http_code=$(curl -s -o /dev/null -w "%{http_code}" \
            -X GET "$CHANGEDETECTION_URL/api/v1/watch" \
            -H "x-api-key: $API_KEY")
          
          if [ "$http_code" != "200" ]; then
            log_failure "Health check failed - API returned HTTP $http_code"
            exit 1
          fi
          
          log_success "Health check passed"
          
          # Ensure directories are writable
          mkdir -p "$LOG_DIR" "$REPORT_DIR" "data/datastore" "data/reports"
          chmod -R 777 "$LOG_DIR" "$REPORT_DIR" "data"
          
          # Run monitoring with retry logic
          for attempt in $(seq 1 "$MAX_MONITOR_ATTEMPTS"); do
            log_info "Monitoring Attempt $attempt/$MAX_MONITOR_ATTEMPTS"
            
            if $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" run --rm \
              -v "$(pwd)/$LOG_DIR:/app/logs" \
              -v "$(pwd)/$REPORT_DIR:/app/data/reports" \
              -v "$(pwd)/data:/app/data" \
              -e CHANGEDETECTION_API_KEY="$API_KEY" \
              -e CHANGEDETECTION_URL="$CHANGEDETECTION_CONTAINER_URL" \
              -e GITHUB_ACTIONS=true \
              -e GITHUB_RUN_ID="$GITHUB_RUN_ID" \
              -e GITHUB_RUN_ATTEMPT="$GITHUB_RUN_ATTEMPT" \
              -e GITHUB_SHA="$GITHUB_SHA" \
              -e GITHUB_REF="$GITHUB_REF" \
              -e MONITOR_RUN_TYPE="${{ github.event.inputs.run_type || 'standard' }}" \
              -e FIRST_RUN="$FIRST_RUN" \
              -e VERBOSE="$VERBOSE" \
              metadata-monitor python run_monitor.py; then
              
              log_success "Monitoring completed successfully on attempt $attempt"
              break
            else
              log_failure "Monitoring failed on attempt $attempt"
              
              if [ "$attempt" -lt "$MAX_MONITOR_ATTEMPTS" ]; then
                log_info "Retrying in ${RESTART_DELAY}s..."
                sleep "$RESTART_DELAY"
                
                log_info "Restarting changedetection service..."
                $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" restart changedetection
                sleep 10
              else
                log_failure "All monitoring attempts failed"
                exit 1
              fi
            fi
          done

      - name: Save service logs
        if: always()
        run: |
          mkdir -p "$DOCKER_LOG_DIR"
          $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" logs changedetection > "$DOCKER_LOG_DIR/changedetection.log" 2>&1 || true
          $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" logs playwright-chrome > "$DOCKER_LOG_DIR/playwright.log" 2>&1 || true

      - name: Upload monitoring reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: monitoring-reports-${{ github.run_number }}
          path: data/reports/
          retention-days: 30
          if-no-files-found: warn

      - name: Upload monitoring data for next run
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: monitoring-data
          path: |
            data/datastore/
            data/reports/
          retention-days: 30
          if-no-files-found: ignore

      - name: Upload service logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: service-logs-${{ github.run_number }}
          path: docker-logs/
          retention-days: 7
          if-no-files-found: ignore

      - name: Collect diagnostics on failure
        if: failure()
        run: |
          source .github/scripts/logger.sh
          
          mkdir -p diagnostics
          
          log_info "Collecting system diagnostics..."
          
          # System info
          uname -a > diagnostics/system-info.txt
          df -h >> diagnostics/system-info.txt
          free -h >> diagnostics/system-info.txt
          
          # Docker status
          docker ps -a > diagnostics/docker-containers.txt 2>&1 || true
          docker network ls > diagnostics/docker-networks.txt 2>&1 || true
          docker volume ls > diagnostics/docker-volumes.txt 2>&1 || true
          
          # Service info
          $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" ps > diagnostics/compose-services.txt 2>&1 || true
          $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" logs --tail=50 > diagnostics/compose-logs.txt 2>&1 || true
          
          # Check if datastore file exists and its content
          if [ -f "$LOCAL_DATASTORE" ]; then
            echo "Datastore file exists, size: $(wc -c < "$LOCAL_DATASTORE") bytes" > diagnostics/datastore-info.txt
            head -c 1000 "$LOCAL_DATASTORE" > diagnostics/datastore-preview.txt 2>&1 || true
          else
            echo "No local datastore file found" > diagnostics/datastore-info.txt
          fi
          
          # Data directory structure
          echo "Data directory structure:" > diagnostics/data-structure.txt
          find data/ -type f -name "*.json" >> diagnostics/data-structure.txt 2>&1 || true
          
          log_success "Diagnostics collected"

      - name: Upload diagnostics
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: failure-diagnostics-${{ github.run_number }}
          path: diagnostics/
          retention-days: 14
          if-no-files-found: ignore

      - name: Cleanup services (preserve data)
        if: always()
        run: |
          source .github/scripts/logger.sh
          
          log_info "Cleaning up services (preserving data)..."
          $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" down --remove-orphans --timeout 30 || true
          
          # Preserve critical data files
          log_info "Preserving datastore and reports for next run..."
          mkdir -p preserved-data/
          cp -r data/datastore preserved-data/ 2>/dev/null || true
          cp -r data/reports preserved-data/ 2>/dev/null || true
          
          # Show what we're preserving
          echo "üìÅ Preserved data:"
          find preserved-data/ -type f -name "*.json" | head -10 || echo "No data to preserve"
          
          log_success "Cleanup completed with data preservation"

      - name: Create workflow summary
        if: always()
        run: |
          {
            echo "## AI Safety Monitor Run Summary"
            echo ""
            echo "**Run Details:**"
            echo "- **Type:** ${{ github.event.inputs.run_type || 'standard' }}"
            echo "- **Branch:** ${{ github.ref_name }}"
            echo "- **Commit:** ${{ github.sha }}"
            echo "- **Timestamp:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
            echo "- **Status:** ${{ job.status }}"
            echo "- **First Run:** ${{ steps.check-first-run.outputs.first_run }}"
            echo ""
            echo "**Artifacts:**"
            if [ "${{ job.status }}" = "success" ]; then
              echo "- ‚úÖ Monitoring reports"
              echo "- ‚úÖ Monitoring data (for next run)"
              echo "- ‚úÖ Service logs"
            else
              echo "- ‚ùå Monitoring reports (may be incomplete)"
              echo "- üìã Monitoring data (preserved for next run)"
              echo "- üìã Service logs"
              echo "- üîç Failure diagnostics"
            fi
            echo ""
            echo "**Data Persistence:**"
            if [ "${{ steps.check-first-run.outputs.first_run }}" = "true" ]; then
              echo "- üÜï First run - initializing datastore"
            else
              echo "- üîÑ Continuing from previous run data"
            fi
          } >> $GITHUB_STEP_SUMMARY