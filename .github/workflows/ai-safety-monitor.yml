name: AI Safety Monitor

on:
  schedule:
    - cron: '0 */6 * * *'  # Run every 6 hours
  workflow_dispatch:
    inputs:
      run_type:
        description: 'Type of run'
        required: true
        default: 'standard'
        type: choice
        options:
          - standard
          - verbose
          - debug

env:
  DOCKER_COMPOSE_PROJECT: ai-safety-monitor-${{ github.run_number }}

jobs:
  monitor:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Debug workspace structure
        run: |
          echo " Current directory structure:"
          ls -la
          echo ""
          echo " .github directory structure:"
          ls -la .github/ || echo "No .github directory"
          echo ""
          echo " Scripts directory structure:"
          ls -la .github/scripts/ || echo "No scripts directory"

      - name: Cache monitoring data
        uses: actions/cache@v4
        id: data-cache
        with:
          path: |
            data/datastore/
            data/reports/
          key: monitoring-data-${{ github.sha }}
          restore-keys: |
            monitoring-data-

      - name: Check cache result and set first run flag
        id: check-first-run
        run: |
          echo " Cache result: ${{ steps.data-cache.outputs.cache-hit }}"
          
          if [[ "${{ steps.data-cache.outputs.cache-hit }}" == 'true' ]]; then
            echo " Cache hit - continuing from previous data"
            echo "FIRST_RUN=false" >> $GITHUB_ENV
            echo "first_run=false" >> $GITHUB_OUTPUT
            
            # Verify the cached data exists
            echo " Cached data contents:"
            find data/ -type f -name "*" 2>/dev/null | head -10 || echo "No cached files found"
          else
            echo " Cache miss - first run detected"
            echo "FIRST_RUN=true" >> $GITHUB_ENV
            echo "first_run=true" >> $GITHUB_OUTPUT
            
            # Initialize data directories
            mkdir -p data/datastore data/reports
            echo " Initialized empty data directories"
          fi

      - name: Restore data permissions and structure
        run: |
          echo " Setting up data directories..."
          mkdir -p data/datastore data/reports data/logs
          chmod -R 777 data/ || true
          
          echo " Existing data content:"
          find data/ -type f -name "*.json" | head -10 || echo "No existing data files"
          echo "Datastore files:"
          ls -la data/datastore/ 2>/dev/null || echo "No datastore files yet"

      - name: Verify prerequisites and scripts
        run: |
          echo " Checking prerequisites..."
          command -v docker && echo " docker found: $(docker --version)" || (echo " docker not found"; exit 1)
          
          # Check for docker compose (plugin) or docker-compose (standalone)
          if docker compose version >/dev/null 2>&1; then
            echo " docker compose (plugin) found: $(docker compose version | head -1)"
            echo "DOCKER_COMPOSE_CMD=docker compose" >> $GITHUB_ENV
          elif command -v docker-compose >/dev/null 2>&1; then
            echo " docker-compose (standalone) found: $(docker-compose --version)"
            echo "DOCKER_COMPOSE_CMD=docker-compose" >> $GITHUB_ENV
          else
            echo " Neither docker compose nor docker-compose found"
            exit 1
          fi
          
          command -v curl && echo " curl found: $(curl --version | head -1)" || (echo " curl not found"; exit 1)
          command -v jq && echo " jq found: $(jq --version)" || (echo " jq not found"; exit 1)
          
          echo ""
          echo " Checking required scripts..."
          required_scripts=(
            ".github/scripts/config.sh"
            ".github/scripts/logger.sh"
            ".github/scripts/wait-for-service.sh"
            ".github/scripts/extract-api-key.sh"
          )
          
          for script in "${required_scripts[@]}"; do
            if [ ! -f "$script" ]; then
              echo " Required script not found: $script"
              exit 1
            fi
            chmod +x "$script"
            echo " $script is executable"
          done
          
          echo " All prerequisites and scripts available"

      - name: Setup environment
        run: |
          # Source config and make available to subsequent steps
          set -a  # Automatically export all variables
          source .github/scripts/config.sh
          set +a
          
          # Export all config variables for subsequent steps
          {
            echo "CHANGEDETECTION_PORT=$CHANGEDETECTION_PORT"
            echo "CHANGEDETECTION_HOST=$CHANGEDETECTION_HOST"
            echo "CHANGEDETECTION_URL=$CHANGEDETECTION_URL"
            echo "CHANGEDETECTION_CONTAINER_URL=$CHANGEDETECTION_CONTAINER_URL"
            echo "MAX_WAIT_ATTEMPTS=$MAX_WAIT_ATTEMPTS"
            echo "WAIT_INTERVAL=$WAIT_INTERVAL"
            echo "MAX_MONITOR_ATTEMPTS=$MAX_MONITOR_ATTEMPTS"
            echo "RESTART_DELAY=$RESTART_DELAY"
            echo "DATASTORE_FILE=$DATASTORE_FILE"
            echo "LOCAL_DATASTORE=$LOCAL_DATASTORE"
            echo "API_KEY_FIELD=$API_KEY_FIELD"
            echo "LOG_DIR=$LOG_DIR"
            echo "DOCKER_LOG_DIR=$DOCKER_LOG_DIR"
            echo "REPORT_DIR=$REPORT_DIR"
          } >> $GITHUB_ENV
          
          echo " Environment configured"
          echo "Service URL: $CHANGEDETECTION_URL"
          echo "Docker Compose Command: $DOCKER_COMPOSE_CMD"

      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          driver: docker-container
          buildkitd-flags: --debug

      - name: Cache Python packages
        uses: actions/cache@v4
        id: pip-cache
        with:
          path: |
            ~/.cache/pip
            /tmp/pip-cache
            **/__pycache__
            **/.pytest_cache
          key: ${{ runner.os }}-python-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-python-

      - name: Cache Docker layers
        uses: actions/cache@v4
        id: docker-cache
        with:
          path: |
            ~/.cache/docker
            /tmp/.buildx-cache
            /var/lib/docker
          key: ${{ runner.os }}-docker-${{ hashFiles('docker-compose.yaml', 'Dockerfile', 'uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-docker-

      - name: Verify cache status
        run: |
          echo " Cache Status:"
          echo "Python cache hit: ${{ steps.pip-cache.outputs.cache-hit }}"
          echo "Docker cache hit: ${{ steps.docker-cache.outputs.cache-hit }}"
          echo "Data cache hit: ${{ steps.data-cache.outputs.cache-hit }}"
          
          if [[ "${{ steps.docker-cache.outputs.cache-hit }}" != 'true' ]]; then
            echo "Docker cache miss - this is normal for first run or when files change"
          else
            echo " Docker cache hit - layers will be reused"
          fi

      - name: Export uv.lock to requirements.txt
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install uv
          # Export a pip-compatible requirements.txt from uv.lock
          uv export --format requirements-txt > requirements.txt
          echo "Exported requirements.txt (head):"
          head -n 50 requirements.txt || true

      - name: Build and start services
        run: |
          source .github/scripts/logger.sh
          
          log_info "Building metadata-monitor image with cache..."
          $DOCKER_COMPOSE_CMD build --progress plain --build-arg BUILDKIT_INLINE_CACHE=1 metadata-monitor
          
          log_info "Starting changedetection and playwright services..."
          $DOCKER_COMPOSE_CMD -p $DOCKER_COMPOSE_PROJECT up -d changedetection playwright-chrome
          
          log_success "Services started successfully"

      - name: Verify bs4 is installed in metadata-monitor image
        run: |
          # Get the image name built by docker-compose (format: {project}-{service})
          IMAGE_NAME="${DOCKER_COMPOSE_PROJECT}-metadata-monitor"
          echo "Verifying bs4 in image: $IMAGE_NAME"
          docker run --rm "$IMAGE_NAME" python -<<'PY'
          import importlib, sys
          try:
              importlib.import_module('bs4')
              print('bs4 import OK')
          except Exception as e:
              print('bs4 import FAILED:', e)
              sys.exit(1)
          PY

      - name: Wait for basic connectivity
        run: |
          .github/scripts/wait-for-service.sh "$CHANGEDETECTION_URL" "$MAX_WAIT_ATTEMPTS" "$WAIT_INTERVAL"

      - name: Extract API key from changedetection datastore
        id: extract-api-key
        run: |
          API_KEY=$(.github/scripts/extract-api-key.sh)
          echo "CHANGEDETECTION_API_KEY=$API_KEY" >> $GITHUB_ENV
          echo "api_key=$API_KEY" >> $GITHUB_OUTPUT
          echo "::add-mask::$API_KEY"

      - name: Test systeminfo endpoint
        env:
          API_KEY: ${{ steps.extract-api-key.outputs.api_key }}
        run: |
          source .github/scripts/logger.sh
          
          log_info "Testing systeminfo endpoint..."
          
          response=$(curl -s -X GET "$CHANGEDETECTION_URL/api/v1/systeminfo" \
            -H "x-api-key: $API_KEY" \
            -w "\n%{http_code}")
          
          http_code=$(echo "$response" | tail -1)
          
          if [ "$http_code" = "200" ]; then
            log_success "systeminfo endpoint returned HTTP 200"
          else
            log_failure "systeminfo endpoint returned HTTP $http_code"
            log_info "Response body:"
            echo "$response" | head -n -1
            exit 1
          fi

      - name: Prepare directories
        run: |
          source .github/scripts/logger.sh
          
          log_info "Creating required directories..."
          mkdir -p "$REPORT_DIR" "$LOG_DIR" "$DOCKER_LOG_DIR" "data/datastore" "data/reports"
          chmod -R 777 "$REPORT_DIR" "$LOG_DIR" "$DOCKER_LOG_DIR" "data"
          log_success "Directories prepared"

      - name: Initialize on first run
        if: steps.check-first-run.outputs.first_run == 'true'
        run: |
          source .github/scripts/logger.sh
          log_info "First run detected - setting up initial configuration"
          
          # Ensure directories exist
          mkdir -p data/datastore data/reports
          
          log_success "First run initialization completed"

      - name: Run monitoring cycle
        env:
          API_KEY: ${{ steps.extract-api-key.outputs.api_key }}
          VERBOSE: ${{ github.event.inputs.run_type == 'verbose' || github.event.inputs.run_type == 'debug' }}
          DEBUG: ${{ github.event.inputs.run_type == 'debug' }}
          GOOGLE_SHEETS_TYPE: ${{ secrets.GOOGLE_SHEETS_TYPE }}
          GOOGLE_SHEETS_PROJECT_ID: ${{ secrets.GOOGLE_SHEETS_PROJECT_ID }}
          GOOGLE_SHEETS_PRIVATE_KEY_ID: ${{ secrets.GOOGLE_SHEETS_PRIVATE_KEY_ID }}
          GOOGLE_SHEETS_PRIVATE_KEY: ${{ secrets.GOOGLE_SHEETS_PRIVATE_KEY }}
          GOOGLE_SHEETS_CLIENT_EMAIL: ${{ secrets.GOOGLE_SHEETS_CLIENT_EMAIL }}
          GOOGLE_SHEETS_CLIENT_ID: ${{ secrets.GOOGLE_SHEETS_CLIENT_ID }}
        run: |
          set -e
          source .github/scripts/logger.sh

          # Get the first_run value directly from the step output
          FIRST_RUN="${{ steps.check-first-run.outputs.first_run }}"
          
          log_info "First run status: $FIRST_RUN"
          log_info "Data cache hit: ${{ steps.data-cache.outputs.cache-hit }}"
          
          # Enable debug mode if requested
          if [ "$DEBUG" = "true" ]; then
            set -x
            export PS4='+ $(date -u "+%H:%M:%S") '
            log_info "DEBUG mode enabled"
          fi
          
          log_info "Performing final health check..."
          http_code=$(curl -s -o /dev/null -w "%{http_code}" \
            -X GET "$CHANGEDETECTION_URL/api/v1/watch" \
            -H "x-api-key: $API_KEY")
          
          if [ "$http_code" != "200" ]; then
            log_failure "Health check failed - API returned HTTP $http_code"
            exit 1
          fi
          
          log_success "Health check passed"
          
          # Ensure directories are writable
          mkdir -p "$LOG_DIR" "$REPORT_DIR" "data/datastore" "data/reports"
          chmod -R 777 "$LOG_DIR" "$REPORT_DIR" "data"
          
          # Run monitoring with retry logic
          for attempt in $(seq 1 "$MAX_MONITOR_ATTEMPTS"); do
            log_info "Monitoring Attempt $attempt/$MAX_MONITOR_ATTEMPTS"
            
            if $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" run --rm \
              -v "$(pwd)/$LOG_DIR:/app/logs" \
              -v "$(pwd)/$REPORT_DIR:/app/data/reports" \
              -v "$(pwd)/data:/app/data" \
              -e CHANGEDETECTION_API_KEY="$API_KEY" \
              -e CHANGEDETECTION_URL="$CHANGEDETECTION_CONTAINER_URL" \
              -e GITHUB_ACTIONS=true \
              -e GITHUB_RUN_ID="$GITHUB_RUN_ID" \
              -e GITHUB_RUN_ATTEMPT="$GITHUB_RUN_ATTEMPT" \
              -e GITHUB_SHA="$GITHUB_SHA" \
              -e GITHUB_REF="$GITHUB_REF" \
              -e MONITOR_RUN_TYPE="${{ github.event.inputs.run_type || 'standard' }}" \
              -e FIRST_RUN="$FIRST_RUN" \
              -e VERBOSE="$VERBOSE" \
              metadata-monitor python run_monitor.py; then
              
              log_success "Monitoring completed successfully on attempt $attempt"
              break
            else
              log_failure "Monitoring failed on attempt $attempt"
              
              if [ "$attempt" -lt "$MAX_MONITOR_ATTEMPTS" ]; then
                log_info "Retrying in ${RESTART_DELAY}s..."
                sleep "$RESTART_DELAY"
                
                log_info "Restarting changedetection service..."
                $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" restart changedetection
                sleep 10
              else
                log_failure "All monitoring attempts failed"
                exit 1
              fi
            fi
          done

      - name: Verify data for caching
        run: |
          echo " Data to be cached for next run:"
          echo "Datastore contents:"
          ls -la data/datastore/ 2>/dev/null || echo "No datastore files"
          echo "Reports contents:"
          ls -la data/reports/ 2>/dev/null || echo "No report files"
          
          if [ -f "data/datastore/changedetection.db" ]; then
            db_size=$(wc -c < "data/datastore/changedetection.db")
            echo " changedetection.db ready for cache: $db_size bytes"
          else
            echo "ï¸ changedetection.db not found - nothing to cache"
          fi

      - name: Save service logs
        if: always()
        run: |
          mkdir -p "$DOCKER_LOG_DIR"
          $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" logs changedetection > "$DOCKER_LOG_DIR/changedetection.log" 2>&1 || true
          $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" logs playwright-chrome > "$DOCKER_LOG_DIR/playwright.log" 2>&1 || true

      - name: Upload monitoring reports (optional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: monitoring-reports-${{ github.run_number }}
          path: data/reports/
          retention-days: 30
          if-no-files-found: warn

      - name: Upload service logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: service-logs-${{ github.run_number }}
          path: docker-logs/
          retention-days: 7
          if-no-files-found: ignore

      - name: Collect diagnostics on failure
        if: failure()
        run: |
          source .github/scripts/logger.sh
          
          mkdir -p diagnostics
          
          log_info "Collecting system diagnostics..."
          
          # System info
          uname -a > diagnostics/system-info.txt
          df -h >> diagnostics/system-info.txt
          free -h >> diagnostics/system-info.txt
          
          # Docker status
          docker ps -a > diagnostics/docker-containers.txt 2>&1 || true
          docker network ls > diagnostics/docker-networks.txt 2>&1 || true
          docker volume ls > diagnostics/docker-volumes.txt 2>&1 || true
          
          # Service info
          $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" ps > diagnostics/compose-services.txt 2>&1 || true
          $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" logs --tail=50 > diagnostics/compose-logs.txt 2>&1 || true
          
          # Check if datastore file exists and its content
          if [ -f "$LOCAL_DATASTORE" ]; then
            echo "Datastore file exists, size: $(wc -c < "$LOCAL_DATASTORE") bytes" > diagnostics/datastore-info.txt
            head -c 1000 "$LOCAL_DATASTORE" > diagnostics/datastore-preview.txt 2>&1 || true
          else
            echo "No local datastore file found" > diagnostics/datastore-info.txt
          fi
          
          # Data directory structure
          echo "Data directory structure:" > diagnostics/data-structure.txt
          find data/ -type f -name "*.json" >> diagnostics/data-structure.txt 2>&1 || true
          
          log_success "Diagnostics collected"

      - name: Upload diagnostics
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: failure-diagnostics-${{ github.run_number }}
          path: diagnostics/
          retention-days: 14
          if-no-files-found: ignore

      - name: Cleanup services (preserve data)
        if: always()
        run: |
          source .github/scripts/logger.sh
          
          log_info "Cleaning up services (preserving data)..."
          $DOCKER_COMPOSE_CMD -p "$DOCKER_COMPOSE_PROJECT" down --remove-orphans --timeout 30 || true
          
          # Show what data we're preserving for cache
          echo " Data preserved for next run cache:"
          find data/ -type f -name "*.db" -o -name "*.json" | head -10 || echo "No data files to preserve"
          
          log_success "Cleanup completed with data preservation"

      - name: Create workflow summary
        if: always()
        run: |
          {
            echo "## AI Safety Monitor Run Summary"
            echo ""
            echo "**Run Details:**"
            echo "- **Type:** ${{ github.event.inputs.run_type || 'standard' }}"
            echo "- **Branch:** ${{ github.ref_name }}"
            echo "- **Commit:** ${{ github.sha }}"
            echo "- **Timestamp:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
            echo "- **Status:** ${{ job.status }}"
            echo "- **First Run:** ${{ steps.check-first-run.outputs.first_run }}"
            echo "- **Data Cache:** ${{ steps.data-cache.outputs.cache-hit }}"
            echo ""
            echo "**Artifacts:**"
            if [ "${{ job.status }}" = "success" ]; then
              echo "-  Monitoring reports"
              echo "-  Service logs"
              echo "-  Data cached for next run"
            else
              echo "-  Monitoring reports (may be incomplete)"
              echo "-  Service logs"
              echo "-  Data cached for next run"
              echo "-  Failure diagnostics"
            fi
            echo ""
            echo "**Data Persistence:**"
            if [ "${{ steps.check-first-run.outputs.first_run }}" = "true" ]; then
              echo "-  First run - initializing datastore"
            else
              echo "-  Continuing from cached data"
            fi
          } >> $GITHUB_STEP_SUMMARY